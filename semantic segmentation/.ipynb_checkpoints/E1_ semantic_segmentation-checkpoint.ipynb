{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from distutils.version import LooseVersion\n",
    "import glob\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import os.path\n",
    "import re\n",
    "import random\n",
    "import scipy.misc\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from urllib.request import urlretrieve\n",
    "import warnings\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_DIRECTORY = './data'\n",
    "RUNS_DIRECTORY = './runs'\n",
    "TRAINING_DATA_DIRECTORY ='./data/data_road/training'\n",
    "NUMBER_OF_IMAGES = len(glob('./data/data_road/training/calib/*.*'))\n",
    "VGG_PATH = './data/vgg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUMBER_OF_CLASSES = 2\n",
    "IMAGE_SHAPE = (160, 576)\n",
    "\n",
    "EPOCHS = 1\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "LEARNING_RATE = 0.0001\n",
    "DROPOUT = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct_label = tf.placeholder(tf.float32, [None,\n",
    "                                            IMAGE_SHAPE[0],\n",
    "                                            IMAGE_SHAPE[1],\n",
    "                                            NUMBER_OF_CLASSES])\n",
    "learning_rate = tf.placeholder(tf.float32)\n",
    "keep_prob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_batch_function(data_folder, image_shape):\n",
    "    def get_batches_fn(batch_size):\n",
    "        image_paths = glob(os.path.join(data_folder, 'image_2', '*.png'))\n",
    "        label_paths = {\n",
    "            re.sub(r'_(lane|road)_', '_', os.path.basename(path)): path\n",
    "            for path in glob(os.path.join(data_folder, 'gt_image_2', '*_road_*.png'))}\n",
    "        background_color = np.array([255, 0, 0])\n",
    "\n",
    "        random.shuffle(image_paths)\n",
    "        for batch_i in range(0, len(image_paths), batch_size):\n",
    "            images = []\n",
    "            gt_images = []\n",
    "            for image_file in image_paths[batch_i:batch_i+batch_size]:\n",
    "                gt_image_file = label_paths[os.path.basename(image_file)]\n",
    "\n",
    "                image = scipy.misc.imresize(scipy.misc.imread(image_file), image_shape)\n",
    "                gt_image = scipy.misc.imresize(scipy.misc.imread(gt_image_file), image_shape)\n",
    "\n",
    "                gt_bg = np.all(gt_image == background_color, axis=2)\n",
    "                gt_bg = gt_bg.reshape(*gt_bg.shape, 1)\n",
    "                gt_image = np.concatenate((gt_bg, np.invert(gt_bg)), axis=2)\n",
    "\n",
    "                images.append(image)\n",
    "                gt_images.append(gt_image)\n",
    "\n",
    "            yield np.array(images), np.array(gt_images)\n",
    "    return get_batches_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_test_output(sess, logits, keep_prob, image_pl, data_folder, image_shape):\n",
    "    for image_file in glob(os.path.join(data_folder, 'image_2', '*.png')):\n",
    "        image = scipy.misc.imresize(scipy.misc.imread(image_file), image_shape)\n",
    "\n",
    "        im_softmax = sess.run(\n",
    "            [tf.nn.softmax(logits)],\n",
    "            {keep_prob: 1.0, image_pl: [image]})\n",
    "        im_softmax = im_softmax[0][:, 1].reshape(image_shape[0], image_shape[1])\n",
    "        segmentation = (im_softmax > 0.5).reshape(image_shape[0], image_shape[1], 1)\n",
    "        mask = np.dot(segmentation, np.array([[0, 255, 0, 127]]))\n",
    "        mask = scipy.misc.toimage(mask, mode=\"RGBA\")\n",
    "        street_im = scipy.misc.toimage(image)\n",
    "        street_im.paste(mask, box=None, mask=mask)\n",
    "\n",
    "        yield os.path.basename(image_file), np.array(street_im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_inference_samples(runs_dir, data_dir, sess, image_shape, logits, keep_prob, input_image):\n",
    "    # Make folder for current run\n",
    "    output_dir = os.path.join(runs_dir, str(time.time()))\n",
    "    if os.path.exists(output_dir):\n",
    "        shutil.rmtree(output_dir)\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "    # Run NN on test images and save them to HD\n",
    "    print('Training Finished. Saving test images to: {}'.format(output_dir))\n",
    "    image_outputs = gen_test_output(\n",
    "        sess, logits, keep_prob, input_image, os.path.join(data_dir, 'data_road/testing'), image_shape)\n",
    "    for name, image in image_outputs:\n",
    "        scipy.misc.imsave(os.path.join(output_dir, name), image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_vgg(sess, vgg_path):\n",
    "    model = tf.saved_model.loader.load(sess, ['vgg16'], vgg_path)\n",
    "\n",
    "    graph = tf.get_default_graph()\n",
    "    image_input = graph.get_tensor_by_name('image_input:0')\n",
    "    keep_prob = graph.get_tensor_by_name('keep_prob:0')\n",
    "    layer3 = graph.get_tensor_by_name('layer3_out:0')\n",
    "    layer4 = graph.get_tensor_by_name('layer4_out:0')\n",
    "    layer7 = graph.get_tensor_by_name('layer7_out:0')\n",
    "\n",
    "    return image_input, keep_prob, layer3, layer4, layer7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def layers(vgg_layer3_out, vgg_layer4_out, vgg_layer7_out, num_classes = NUMBER_OF_CLASSES):\n",
    "    layer3x = tf.layers.conv2d(inputs = vgg_layer3_out,\n",
    "                               filters =  NUMBER_OF_CLASSES,\n",
    "                               kernel_size = (1, 1),\n",
    "                               strides = (1, 1),\n",
    "                               name = \"layer3conv1x1\")\n",
    "    layer4x = tf.layers.conv2d(inputs = vgg_layer4_out,\n",
    "                               filters =  NUMBER_OF_CLASSES,\n",
    "                               kernel_size = (1, 1),\n",
    "                               strides = (1, 1),\n",
    "                               name = \"layer4conv1x1\")\n",
    "    layer7x = tf.layers.conv2d(inputs = vgg_layer7_out,\n",
    "                               filters =  NUMBER_OF_CLASSES,\n",
    "                               kernel_size = (1, 1),\n",
    "                               strides = (1, 1),\n",
    "                               name = \"layer7conv1x1\")\n",
    "\n",
    "    decoderlayer1 = tf.layers.conv2d_transpose(inputs = layer7x,\n",
    "                                               filters = NUMBER_OF_CLASSES,\n",
    "                                               kernel_size = (4, 4),\n",
    "                                               strides = (2, 2),\n",
    "                                               padding = 'same',\n",
    "                                               name = \"decoderlayer1\")\n",
    "    decoderlayer2 = tf.add(decoderlayer1, layer4x, name = \"decoderlayer2\")\n",
    "    decoderlayer3 = tf.layers.conv2d_transpose(inputs = decoderlayer2,\n",
    "                                               filters = NUMBER_OF_CLASSES,\n",
    "                                               kernel_size = (4, 4),\n",
    "                                               strides = (2, 2),\n",
    "                                               padding = 'same',\n",
    "                                               name = \"decoderlayer3\")\n",
    "    decoderlayer4 = tf.add(decoderlayer3, layer3x, name = \"decoderlayer4\")\n",
    "    decoderlayer_output = tf.layers.conv2d_transpose(inputs = decoderlayer4,\n",
    "                                                     filters = NUMBER_OF_CLASSES,\n",
    "                                                     kernel_size = (16, 16),\n",
    "                                                     strides = (8, 8),\n",
    "                                                     padding = 'same',\n",
    "                                                     name = \"decoderlayer_output\")\n",
    "\n",
    "    return decoderlayer_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimize(nn_last_layer, correct_label, learning_rate, num_classes = NUMBER_OF_CLASSES):\n",
    "    logits = tf.reshape(nn_last_layer, (-1, num_classes))\n",
    "    class_labels = tf.reshape(correct_label, (-1, num_classes))\n",
    "\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = class_labels)\n",
    "    cross_entropy_loss = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy_loss)\n",
    "\n",
    "    return logits, train_op, cross_entropy_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_nn(sess, epochs, batch_size, get_batches_fn, train_op, cross_entropy_loss, input_image, correct_label, keep_prob, learning_rate):\n",
    "    for epoch in range(EPOCHS):\n",
    "        losses, i = [], 0\n",
    "        for images, labels in get_batches_fn(BATCH_SIZE):\n",
    "            i += 1\n",
    "            feed = {input_image: images,\n",
    "                   correct_label: labels,\n",
    "                   keep_prob: DROPOUT,\n",
    "                   learning_rate: LEARNING_RATE}\n",
    "\n",
    "            _, partial_loss = sess.run([train_op, cross_entropy_loss], feed_dict = feed)\n",
    "            \n",
    "            print(\"---> iteration: \", i, \"/\", NUMBER_OF_IMAGES, \" partial loss:\", partial_loss)\n",
    "            losses.append(partial_loss)\n",
    "\n",
    "        training_loss = sum(losses) / len(losses)\n",
    "\n",
    "        print(\"------------------\")\n",
    "        print(\"epoch: \", epoch + 1, \"/\", EPOCHS, \"training loss: \", training_loss)\n",
    "        print(\"------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_batches_fn = gen_batch_function(TRAINING_DATA_DIRECTORY, IMAGE_SHAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBER OF IMAGES: 289\n",
      "INFO:tensorflow:Restoring parameters from b'./data/vgg/variables/variables'\n",
      "WARNING:tensorflow:From <ipython-input-10-bcf1567b1fd2>:5: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "---> iteration:  1 / 289  partial loss: 44.5999\n",
      "---> iteration:  2 / 289  partial loss: 42.6249\n",
      "---> iteration:  3 / 289  partial loss: 28.3005\n",
      "---> iteration:  4 / 289  partial loss: 27.0749\n",
      "---> iteration:  5 / 289  partial loss: 23.1804\n",
      "---> iteration:  6 / 289  partial loss: 16.222\n",
      "---> iteration:  7 / 289  partial loss: 20.41\n",
      "---> iteration:  8 / 289  partial loss: 16.258\n",
      "---> iteration:  9 / 289  partial loss: 13.3424\n",
      "---> iteration:  10 / 289  partial loss: 15.1122\n",
      "---> iteration:  11 / 289  partial loss: 15.8117\n",
      "---> iteration:  12 / 289  partial loss: 12.153\n",
      "---> iteration:  13 / 289  partial loss: 11.7406\n",
      "---> iteration:  14 / 289  partial loss: 11.4087\n",
      "---> iteration:  15 / 289  partial loss: 7.89838\n",
      "---> iteration:  16 / 289  partial loss: 8.00959\n",
      "---> iteration:  17 / 289  partial loss: 7.1021\n",
      "---> iteration:  18 / 289  partial loss: 6.36449\n",
      "---> iteration:  19 / 289  partial loss: 5.49636\n",
      "---> iteration:  20 / 289  partial loss: 5.6452\n",
      "---> iteration:  21 / 289  partial loss: 5.85352\n",
      "---> iteration:  22 / 289  partial loss: 6.29319\n",
      "---> iteration:  23 / 289  partial loss: 4.17789\n",
      "---> iteration:  24 / 289  partial loss: 5.18111\n",
      "---> iteration:  25 / 289  partial loss: 5.09701\n",
      "---> iteration:  26 / 289  partial loss: 4.37027\n",
      "---> iteration:  27 / 289  partial loss: 4.07088\n",
      "---> iteration:  28 / 289  partial loss: 3.74125\n",
      "---> iteration:  29 / 289  partial loss: 4.30387\n",
      "---> iteration:  30 / 289  partial loss: 2.69294\n",
      "---> iteration:  31 / 289  partial loss: 2.6418\n",
      "---> iteration:  32 / 289  partial loss: 3.14118\n",
      "---> iteration:  33 / 289  partial loss: 3.08512\n",
      "---> iteration:  34 / 289  partial loss: 2.95965\n",
      "---> iteration:  35 / 289  partial loss: 2.30336\n",
      "---> iteration:  36 / 289  partial loss: 2.63388\n",
      "---> iteration:  37 / 289  partial loss: 2.77261\n",
      "---> iteration:  38 / 289  partial loss: 2.25366\n",
      "---> iteration:  39 / 289  partial loss: 2.16013\n",
      "---> iteration:  40 / 289  partial loss: 2.49661\n",
      "---> iteration:  41 / 289  partial loss: 2.56605\n",
      "---> iteration:  42 / 289  partial loss: 2.12713\n",
      "---> iteration:  43 / 289  partial loss: 1.88899\n",
      "---> iteration:  44 / 289  partial loss: 2.09221\n",
      "---> iteration:  45 / 289  partial loss: 1.73623\n",
      "---> iteration:  46 / 289  partial loss: 1.92239\n",
      "---> iteration:  47 / 289  partial loss: 1.72998\n",
      "---> iteration:  48 / 289  partial loss: 1.46541\n",
      "---> iteration:  49 / 289  partial loss: 1.66354\n",
      "---> iteration:  50 / 289  partial loss: 1.46736\n",
      "---> iteration:  51 / 289  partial loss: 1.44904\n",
      "---> iteration:  52 / 289  partial loss: 1.51875\n",
      "---> iteration:  53 / 289  partial loss: 1.44251\n",
      "---> iteration:  54 / 289  partial loss: 1.712\n",
      "---> iteration:  55 / 289  partial loss: 1.5676\n",
      "---> iteration:  56 / 289  partial loss: 1.27696\n",
      "---> iteration:  57 / 289  partial loss: 1.52463\n",
      "---> iteration:  58 / 289  partial loss: 1.3416\n",
      "---> iteration:  59 / 289  partial loss: 1.24293\n",
      "---> iteration:  60 / 289  partial loss: 1.48377\n",
      "---> iteration:  61 / 289  partial loss: 1.34407\n",
      "---> iteration:  62 / 289  partial loss: 1.18777\n",
      "---> iteration:  63 / 289  partial loss: 1.41536\n",
      "---> iteration:  64 / 289  partial loss: 1.22331\n",
      "---> iteration:  65 / 289  partial loss: 1.3812\n",
      "---> iteration:  66 / 289  partial loss: 1.27413\n",
      "---> iteration:  67 / 289  partial loss: 1.22261\n",
      "---> iteration:  68 / 289  partial loss: 1.12648\n",
      "---> iteration:  69 / 289  partial loss: 1.46949\n",
      "---> iteration:  70 / 289  partial loss: 1.20021\n",
      "---> iteration:  71 / 289  partial loss: 1.13987\n",
      "---> iteration:  72 / 289  partial loss: 1.10601\n",
      "---> iteration:  73 / 289  partial loss: 1.17327\n",
      "---> iteration:  74 / 289  partial loss: 1.07518\n",
      "---> iteration:  75 / 289  partial loss: 1.00497\n",
      "---> iteration:  76 / 289  partial loss: 1.08151\n",
      "---> iteration:  77 / 289  partial loss: 1.08656\n",
      "---> iteration:  78 / 289  partial loss: 1.18598\n",
      "---> iteration:  79 / 289  partial loss: 1.09526\n",
      "---> iteration:  80 / 289  partial loss: 1.01369\n",
      "---> iteration:  81 / 289  partial loss: 1.07054\n",
      "---> iteration:  82 / 289  partial loss: 1.28878\n",
      "---> iteration:  83 / 289  partial loss: 1.15435\n",
      "---> iteration:  84 / 289  partial loss: 0.929337\n",
      "---> iteration:  85 / 289  partial loss: 1.10891\n",
      "---> iteration:  86 / 289  partial loss: 0.960682\n",
      "---> iteration:  87 / 289  partial loss: 0.923662\n",
      "---> iteration:  88 / 289  partial loss: 0.884135\n",
      "---> iteration:  89 / 289  partial loss: 1.01092\n",
      "---> iteration:  90 / 289  partial loss: 1.01053\n",
      "---> iteration:  91 / 289  partial loss: 0.906923\n",
      "---> iteration:  92 / 289  partial loss: 1.03789\n",
      "---> iteration:  93 / 289  partial loss: 0.898555\n",
      "---> iteration:  94 / 289  partial loss: 0.943507\n",
      "---> iteration:  95 / 289  partial loss: 0.914134\n",
      "---> iteration:  96 / 289  partial loss: 1.10753\n",
      "---> iteration:  97 / 289  partial loss: 0.916138\n",
      "---> iteration:  98 / 289  partial loss: 0.88331\n",
      "---> iteration:  99 / 289  partial loss: 1.01139\n",
      "---> iteration:  100 / 289  partial loss: 0.928423\n",
      "---> iteration:  101 / 289  partial loss: 0.873484\n",
      "---> iteration:  102 / 289  partial loss: 1.00838\n",
      "---> iteration:  103 / 289  partial loss: 0.965201\n",
      "---> iteration:  104 / 289  partial loss: 0.895756\n",
      "---> iteration:  105 / 289  partial loss: 0.915464\n",
      "---> iteration:  106 / 289  partial loss: 0.90008\n",
      "---> iteration:  107 / 289  partial loss: 0.888453\n",
      "---> iteration:  108 / 289  partial loss: 0.834372\n",
      "---> iteration:  109 / 289  partial loss: 0.944918\n",
      "---> iteration:  110 / 289  partial loss: 0.929131\n",
      "---> iteration:  111 / 289  partial loss: 0.988306\n",
      "---> iteration:  112 / 289  partial loss: 0.874828\n",
      "---> iteration:  113 / 289  partial loss: 0.794914\n",
      "---> iteration:  114 / 289  partial loss: 0.850101\n",
      "---> iteration:  115 / 289  partial loss: 0.902959\n",
      "---> iteration:  116 / 289  partial loss: 1.07147\n",
      "---> iteration:  117 / 289  partial loss: 0.892787\n",
      "---> iteration:  118 / 289  partial loss: 0.949839\n",
      "---> iteration:  119 / 289  partial loss: 0.834326\n",
      "---> iteration:  120 / 289  partial loss: 0.78812\n",
      "---> iteration:  121 / 289  partial loss: 0.849792\n",
      "---> iteration:  122 / 289  partial loss: 0.810773\n",
      "---> iteration:  123 / 289  partial loss: 0.883733\n",
      "---> iteration:  124 / 289  partial loss: 0.84111\n",
      "---> iteration:  125 / 289  partial loss: 0.846735\n",
      "---> iteration:  126 / 289  partial loss: 0.902478\n",
      "---> iteration:  127 / 289  partial loss: 0.88365\n",
      "---> iteration:  128 / 289  partial loss: 0.828897\n",
      "---> iteration:  129 / 289  partial loss: 0.809833\n",
      "---> iteration:  130 / 289  partial loss: 0.800844\n",
      "---> iteration:  131 / 289  partial loss: 0.743081\n",
      "---> iteration:  132 / 289  partial loss: 0.752825\n",
      "---> iteration:  133 / 289  partial loss: 0.783615\n",
      "---> iteration:  134 / 289  partial loss: 0.81133\n",
      "---> iteration:  135 / 289  partial loss: 0.750232\n",
      "---> iteration:  136 / 289  partial loss: 0.820146\n",
      "---> iteration:  137 / 289  partial loss: 0.85384\n",
      "---> iteration:  138 / 289  partial loss: 0.816188\n",
      "---> iteration:  139 / 289  partial loss: 0.823339\n",
      "---> iteration:  140 / 289  partial loss: 0.794823\n",
      "---> iteration:  141 / 289  partial loss: 1.01117\n",
      "---> iteration:  142 / 289  partial loss: 0.736016\n",
      "---> iteration:  143 / 289  partial loss: 0.822388\n",
      "---> iteration:  144 / 289  partial loss: 0.864807\n",
      "---> iteration:  145 / 289  partial loss: 0.761676\n",
      "---> iteration:  146 / 289  partial loss: 0.797299\n",
      "---> iteration:  147 / 289  partial loss: 0.794823\n",
      "---> iteration:  148 / 289  partial loss: 0.798957\n",
      "---> iteration:  149 / 289  partial loss: 0.753605\n",
      "---> iteration:  150 / 289  partial loss: 0.767175\n",
      "---> iteration:  151 / 289  partial loss: 0.776839\n",
      "---> iteration:  152 / 289  partial loss: 0.765372\n",
      "---> iteration:  153 / 289  partial loss: 0.801665\n",
      "---> iteration:  154 / 289  partial loss: 0.806853\n",
      "---> iteration:  155 / 289  partial loss: 0.818487\n",
      "---> iteration:  156 / 289  partial loss: 0.811826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> iteration:  157 / 289  partial loss: 0.828315\n",
      "---> iteration:  158 / 289  partial loss: 0.780988\n",
      "---> iteration:  159 / 289  partial loss: 0.862829\n",
      "---> iteration:  160 / 289  partial loss: 0.748644\n",
      "---> iteration:  161 / 289  partial loss: 0.755717\n",
      "---> iteration:  162 / 289  partial loss: 0.757347\n",
      "---> iteration:  163 / 289  partial loss: 0.723718\n",
      "---> iteration:  164 / 289  partial loss: 0.744351\n",
      "---> iteration:  165 / 289  partial loss: 0.723402\n",
      "---> iteration:  166 / 289  partial loss: 0.747845\n",
      "---> iteration:  167 / 289  partial loss: 0.787525\n",
      "---> iteration:  168 / 289  partial loss: 0.818439\n",
      "---> iteration:  169 / 289  partial loss: 0.817782\n",
      "---> iteration:  170 / 289  partial loss: 0.713169\n",
      "---> iteration:  171 / 289  partial loss: 0.713084\n",
      "---> iteration:  172 / 289  partial loss: 0.693184\n",
      "---> iteration:  173 / 289  partial loss: 0.724623\n",
      "---> iteration:  174 / 289  partial loss: 0.756042\n",
      "---> iteration:  175 / 289  partial loss: 0.712215\n",
      "---> iteration:  176 / 289  partial loss: 0.726481\n",
      "---> iteration:  177 / 289  partial loss: 0.756971\n",
      "---> iteration:  178 / 289  partial loss: 0.756381\n",
      "---> iteration:  179 / 289  partial loss: 0.705878\n",
      "---> iteration:  180 / 289  partial loss: 0.777845\n",
      "---> iteration:  181 / 289  partial loss: 0.750709\n",
      "---> iteration:  182 / 289  partial loss: 0.73471\n",
      "---> iteration:  183 / 289  partial loss: 0.863036\n",
      "---> iteration:  184 / 289  partial loss: 0.71841\n",
      "---> iteration:  185 / 289  partial loss: 0.809162\n",
      "---> iteration:  186 / 289  partial loss: 0.703863\n",
      "---> iteration:  187 / 289  partial loss: 0.725287\n",
      "---> iteration:  188 / 289  partial loss: 0.779995\n",
      "---> iteration:  189 / 289  partial loss: 0.762608\n",
      "---> iteration:  190 / 289  partial loss: 0.748913\n",
      "---> iteration:  191 / 289  partial loss: 0.699499\n",
      "---> iteration:  192 / 289  partial loss: 0.684144\n",
      "---> iteration:  193 / 289  partial loss: 0.716878\n",
      "---> iteration:  194 / 289  partial loss: 0.701856\n",
      "---> iteration:  195 / 289  partial loss: 0.670768\n",
      "---> iteration:  196 / 289  partial loss: 0.684604\n",
      "---> iteration:  197 / 289  partial loss: 0.794784\n",
      "---> iteration:  198 / 289  partial loss: 0.708593\n",
      "---> iteration:  199 / 289  partial loss: 0.714967\n",
      "---> iteration:  200 / 289  partial loss: 0.677252\n",
      "---> iteration:  201 / 289  partial loss: 0.680486\n",
      "---> iteration:  202 / 289  partial loss: 0.705742\n",
      "---> iteration:  203 / 289  partial loss: 0.745888\n",
      "---> iteration:  204 / 289  partial loss: 0.683948\n",
      "---> iteration:  205 / 289  partial loss: 0.721912\n",
      "---> iteration:  206 / 289  partial loss: 0.691657\n",
      "---> iteration:  207 / 289  partial loss: 0.725985\n",
      "---> iteration:  208 / 289  partial loss: 0.702494\n",
      "---> iteration:  209 / 289  partial loss: 0.696582\n",
      "---> iteration:  210 / 289  partial loss: 0.690698\n",
      "---> iteration:  211 / 289  partial loss: 0.713996\n",
      "---> iteration:  212 / 289  partial loss: 0.670053\n",
      "---> iteration:  213 / 289  partial loss: 0.693647\n",
      "---> iteration:  214 / 289  partial loss: 0.728974\n",
      "---> iteration:  215 / 289  partial loss: 0.702647\n",
      "---> iteration:  216 / 289  partial loss: 0.687745\n",
      "---> iteration:  217 / 289  partial loss: 0.768628\n",
      "---> iteration:  218 / 289  partial loss: 0.681084\n",
      "---> iteration:  219 / 289  partial loss: 0.67263\n",
      "---> iteration:  220 / 289  partial loss: 0.723278\n",
      "---> iteration:  221 / 289  partial loss: 0.692114\n",
      "---> iteration:  222 / 289  partial loss: 0.643873\n",
      "---> iteration:  223 / 289  partial loss: 0.701993\n",
      "---> iteration:  224 / 289  partial loss: 0.671502\n",
      "---> iteration:  225 / 289  partial loss: 0.646767\n",
      "---> iteration:  226 / 289  partial loss: 0.669603\n",
      "---> iteration:  227 / 289  partial loss: 0.687334\n",
      "---> iteration:  228 / 289  partial loss: 0.660704\n",
      "---> iteration:  229 / 289  partial loss: 0.688622\n",
      "---> iteration:  230 / 289  partial loss: 0.727507\n",
      "---> iteration:  231 / 289  partial loss: 0.641577\n",
      "---> iteration:  232 / 289  partial loss: 0.672113\n",
      "---> iteration:  233 / 289  partial loss: 0.663436\n",
      "---> iteration:  234 / 289  partial loss: 0.703379\n",
      "---> iteration:  235 / 289  partial loss: 0.649393\n",
      "---> iteration:  236 / 289  partial loss: 0.651781\n",
      "---> iteration:  237 / 289  partial loss: 0.684044\n",
      "---> iteration:  238 / 289  partial loss: 0.670297\n",
      "---> iteration:  239 / 289  partial loss: 0.666241\n",
      "---> iteration:  240 / 289  partial loss: 0.715845\n",
      "---> iteration:  241 / 289  partial loss: 0.675523\n",
      "---> iteration:  242 / 289  partial loss: 0.656\n",
      "---> iteration:  243 / 289  partial loss: 0.654062\n",
      "---> iteration:  244 / 289  partial loss: 0.678446\n",
      "---> iteration:  245 / 289  partial loss: 0.67671\n",
      "---> iteration:  246 / 289  partial loss: 0.674935\n",
      "---> iteration:  247 / 289  partial loss: 0.681379\n",
      "---> iteration:  248 / 289  partial loss: 0.671222\n",
      "---> iteration:  249 / 289  partial loss: 0.656857\n",
      "---> iteration:  250 / 289  partial loss: 0.647386\n",
      "---> iteration:  251 / 289  partial loss: 0.679603\n",
      "---> iteration:  252 / 289  partial loss: 0.661201\n",
      "---> iteration:  253 / 289  partial loss: 0.717078\n",
      "---> iteration:  254 / 289  partial loss: 0.662758\n",
      "---> iteration:  255 / 289  partial loss: 0.642618\n",
      "---> iteration:  256 / 289  partial loss: 0.654972\n",
      "---> iteration:  257 / 289  partial loss: 0.64662\n",
      "---> iteration:  258 / 289  partial loss: 0.667484\n",
      "---> iteration:  259 / 289  partial loss: 0.672286\n",
      "---> iteration:  260 / 289  partial loss: 0.686398\n",
      "---> iteration:  261 / 289  partial loss: 0.631952\n",
      "---> iteration:  262 / 289  partial loss: 0.662351\n",
      "---> iteration:  263 / 289  partial loss: 0.639107\n",
      "---> iteration:  264 / 289  partial loss: 0.665636\n",
      "---> iteration:  265 / 289  partial loss: 0.652494\n",
      "---> iteration:  266 / 289  partial loss: 0.623572\n",
      "---> iteration:  267 / 289  partial loss: 0.631228\n",
      "---> iteration:  268 / 289  partial loss: 0.623292\n",
      "---> iteration:  269 / 289  partial loss: 0.652939\n",
      "---> iteration:  270 / 289  partial loss: 0.654476\n",
      "---> iteration:  271 / 289  partial loss: 0.660736\n",
      "---> iteration:  272 / 289  partial loss: 0.664805\n",
      "---> iteration:  273 / 289  partial loss: 0.64133\n",
      "---> iteration:  274 / 289  partial loss: 0.650302\n",
      "---> iteration:  275 / 289  partial loss: 0.666411\n",
      "---> iteration:  276 / 289  partial loss: 0.655667\n",
      "---> iteration:  277 / 289  partial loss: 0.669783\n",
      "---> iteration:  278 / 289  partial loss: 0.650041\n",
      "---> iteration:  279 / 289  partial loss: 0.630585\n",
      "---> iteration:  280 / 289  partial loss: 0.626102\n",
      "---> iteration:  281 / 289  partial loss: 0.631162\n",
      "---> iteration:  282 / 289  partial loss: 0.626836\n",
      "---> iteration:  283 / 289  partial loss: 0.64797\n",
      "---> iteration:  284 / 289  partial loss: 0.619061\n",
      "---> iteration:  285 / 289  partial loss: 0.646186\n",
      "---> iteration:  286 / 289  partial loss: 0.604967\n",
      "---> iteration:  287 / 289  partial loss: 0.624165\n",
      "---> iteration:  288 / 289  partial loss: 0.63072\n",
      "---> iteration:  289 / 289  partial loss: 0.617696\n",
      "------------------\n",
      "epoch:  1  of  1 training loss:  2.17177888225\n",
      "------------------\n",
      "Training Finished. Saving test images to: ./runs/1\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './runs/1/uu_000058.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-e503c972ea4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m              correct_label, keep_prob, learning_rate)\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0msave_inference_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRUNS_DIRECTORY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDATA_DIRECTORY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMAGE_SHAPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-266b4c96d382>\u001b[0m in \u001b[0;36msave_inference_samples\u001b[0;34m(runs_dir, epochs, data_dir, sess, image_shape, logits, keep_prob, input_image)\u001b[0m\n\u001b[1;32m      6\u001b[0m         sess, logits, keep_prob, input_image, os.path.join(data_dir, 'data_road/testing'), image_shape)\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimage_outputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmisc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimsave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/scipy/misc/pilutil.py\u001b[0m in \u001b[0;36mimsave\u001b[0;34m(name, arr, format)\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoimage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannel_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   1888\u001b[0m             \u001b[0;31m# Open also for reading (\"+\"), because TIFF save_all\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1889\u001b[0m             \u001b[0;31m# writer needs to go back and edit the written data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1890\u001b[0;31m             \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w+b\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1891\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1892\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './runs/1/uu_000058.png'"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:    \n",
    "    image_input, keep_prob, layer3, layer4, layer7 = load_vgg(session, VGG_PATH)\n",
    "    model_output = layers(layer3, layer4, layer7, NUMBER_OF_CLASSES)\n",
    "\n",
    "    logits, train_op, cross_entropy_loss = optimize(model_output, correct_label, learning_rate, NUMBER_OF_CLASSES)\n",
    "    \n",
    "    session.run([tf.global_variables_initializer(), tf.local_variables_initializer()])\n",
    "\n",
    "    train_nn(session, EPOCHS, BATCH_SIZE, get_batches_fn, \n",
    "             train_op, cross_entropy_loss, image_input,\n",
    "             correct_label, keep_prob, learning_rate)\n",
    "\n",
    "    save_inference_samples(RUNS_DIRECTORY, EPOCHS, DATA_DIRECTORY, session, IMAGE_SHAPE, logits, keep_prob, image_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
