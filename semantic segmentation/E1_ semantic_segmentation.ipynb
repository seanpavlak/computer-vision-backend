{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from distutils.version import LooseVersion\n",
    "import glob\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import os.path\n",
    "import re\n",
    "import random\n",
    "import scipy.misc\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from urllib.request import urlretrieve\n",
    "import warnings\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_DIRECTORY = './data'\n",
    "RUNS_DIRECTORY = './runs'\n",
    "TRAINING_DATA_DIRECTORY ='./data/data_road/training'\n",
    "NUMBER_OF_IMAGES = len(glob('./data/data_road/training/calib/*.*'))\n",
    "VGG_PATH = './data/vgg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUMBER_OF_CLASSES = 2\n",
    "IMAGE_SHAPE = (160, 576)\n",
    "\n",
    "EPOCHS = 1\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "LEARNING_RATE = 0.0001\n",
    "DROPOUT = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct_label = tf.placeholder(tf.float32, [None, IMAGE_SHAPE[0], IMAGE_SHAPE[1], NUMBER_OF_CLASSES])\n",
    "learning_rate = tf.placeholder(tf.float32)\n",
    "keep_prob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_batch_function(data_folder, image_shape):\n",
    "    def get_batches_function(batch_size):\n",
    "        image_paths = glob(os.path.join(data_folder, 'image_2', '*.png'))\n",
    "        label_paths = {\n",
    "            re.sub(r'_(lane|road)_', '_', os.path.basename(path)): path\n",
    "            for path in glob(os.path.join(data_folder, 'gt_image_2', '*_road_*.png'))}\n",
    "        background_color = np.array([255, 0, 0])\n",
    "        random.shuffle(image_paths)\n",
    "        \n",
    "        for i in range(0, len(image_paths), batch_size):\n",
    "            images = []\n",
    "            gt_images = []\n",
    "            \n",
    "            for image_file in image_paths[i:i+batch_size]:\n",
    "                gt_image_file = label_paths[os.path.basename(image_file)]\n",
    "\n",
    "                image = scipy.misc.imresize(scipy.misc.imread(image_file), image_shape)\n",
    "                gt_image = scipy.misc.imresize(scipy.misc.imread(gt_image_file), image_shape)\n",
    "\n",
    "                gt_bg = np.all(gt_image == background_color, axis=2)\n",
    "                gt_bg = gt_bg.reshape(*gt_bg.shape, 1)\n",
    "                gt_image = np.concatenate((gt_bg, np.invert(gt_bg)), axis=2)\n",
    "\n",
    "                images.append(image)\n",
    "                gt_images.append(gt_image)\n",
    "\n",
    "            yield np.array(images), np.array(gt_images)\n",
    "    return get_batches_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_test_output(sess, logits, keep_prob, image_pl, data_folder, image_shape):\n",
    "    for image_file in glob(os.path.join(data_folder, 'image_2', '*.png')):\n",
    "        image = scipy.misc.imresize(scipy.misc.imread(image_file), image_shape)\n",
    "\n",
    "        im_softmax = sess.run([tf.nn.softmax(logits)], {keep_prob: 1.0, image_pl: [image]})\n",
    "        im_softmax = im_softmax[0][:, 1].reshape(image_shape[0], image_shape[1])\n",
    "        \n",
    "        segmentation = (im_softmax > 0.5).reshape(image_shape[0], image_shape[1], 1)\n",
    "        \n",
    "        mask = np.dot(segmentation, np.array([[0, 255, 0, 127]]))\n",
    "        mask = scipy.misc.toimage(mask, mode='RGBA')\n",
    "        \n",
    "        street_im = scipy.misc.toimage(image)\n",
    "        street_im.paste(mask, box=None, mask=mask)\n",
    "\n",
    "        yield os.path.basename(image_file), np.array(street_im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_inference_samples(runs_dir, data_dir, sess, image_shape, logits, keep_prob, input_image):\n",
    "    output_dir = os.path.join(runs_dir, 'epochs {0}'.format(EPOCHS))\n",
    "    \n",
    "    if os.path.exists(output_dir):\n",
    "        shutil.rmtree(output_dir)\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "    print('Training Finished. Saving test images to: {}'.format(output_dir))\n",
    "    image_outputs = generate_test_output(sess, logits, keep_prob, input_image, os.path.join(data_dir, 'data_road/testing'), image_shape)\n",
    "    \n",
    "    for name, image in image_outputs:\n",
    "        scipy.misc.imsave(os.path.join(output_dir, name), image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_vgg(sess, vgg_path):\n",
    "    model = tf.saved_model.loader.load(sess, ['vgg16'], vgg_path)\n",
    "\n",
    "    graph = tf.get_default_graph()\n",
    "    image_input = graph.get_tensor_by_name('image_input:0')\n",
    "    keep_prob = graph.get_tensor_by_name('keep_prob:0')\n",
    "    layer3 = graph.get_tensor_by_name('layer3_out:0')\n",
    "    layer4 = graph.get_tensor_by_name('layer4_out:0')\n",
    "    layer7 = graph.get_tensor_by_name('layer7_out:0')\n",
    "\n",
    "    return image_input, keep_prob, layer3, layer4, layer7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def layers(vgg_layer3_out, vgg_layer4_out, vgg_layer7_out, num_classes = NUMBER_OF_CLASSES):\n",
    "    layer3x = tf.layers.conv2d(inputs = vgg_layer3_out,\n",
    "                               filters =  NUMBER_OF_CLASSES,\n",
    "                               kernel_size = (1, 1),\n",
    "                               strides = (1, 1),\n",
    "                               name = 'layer3conv1x1')\n",
    "    \n",
    "    layer4x = tf.layers.conv2d(inputs = vgg_layer4_out,\n",
    "                               filters =  NUMBER_OF_CLASSES,\n",
    "                               kernel_size = (1, 1),\n",
    "                               strides = (1, 1),\n",
    "                               name = 'layer4conv1x1')\n",
    "    \n",
    "    layer7x = tf.layers.conv2d(inputs = vgg_layer7_out,\n",
    "                               filters =  NUMBER_OF_CLASSES,\n",
    "                               kernel_size = (1, 1),\n",
    "                               strides = (1, 1),\n",
    "                               name = 'layer7conv1x1')\n",
    "\n",
    "    decoderlayer1 = tf.layers.conv2d_transpose(inputs = layer7x,\n",
    "                                               filters = NUMBER_OF_CLASSES,\n",
    "                                               kernel_size = (4, 4),\n",
    "                                               strides = (2, 2),\n",
    "                                               padding = 'same',\n",
    "                                               name = 'decoderlayer1')\n",
    "    decoderlayer2 = tf.add(decoderlayer1, layer4x, name = 'decoderlayer2')\n",
    "    \n",
    "    decoderlayer3 = tf.layers.conv2d_transpose(inputs = decoderlayer2,\n",
    "                                               filters = NUMBER_OF_CLASSES,\n",
    "                                               kernel_size = (4, 4),\n",
    "                                               strides = (2, 2),\n",
    "                                               padding = 'same',\n",
    "                                               name = 'decoderlayer3')\n",
    "    \n",
    "    decoderlayer4 = tf.add(decoderlayer3, layer3x, name = 'decoderlayer4')\n",
    "    decoderlayer_output = tf.layers.conv2d_transpose(inputs = decoderlayer4,\n",
    "                                                     filters = NUMBER_OF_CLASSES,\n",
    "                                                     kernel_size = (16, 16),\n",
    "                                                     strides = (8, 8),\n",
    "                                                     padding = 'same',\n",
    "                                                     name = 'decoderlayer_output')\n",
    "\n",
    "    return decoderlayer_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimize(nn_last_layer, correct_label, learning_rate, num_classes = NUMBER_OF_CLASSES):\n",
    "    logits = tf.reshape(nn_last_layer, (-1, num_classes))\n",
    "    class_labels = tf.reshape(correct_label, (-1, num_classes))\n",
    "\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits = logits, labels = class_labels)\n",
    "    cross_entropy_loss = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "    train_optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy_loss)\n",
    "\n",
    "    return logits, train_optimizer, cross_entropy_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_nn(sess, epochs, batch_size, get_batches_function, train_optimizer, cross_entropy_loss, input_image, correct_label, keep_prob, learning_rate):\n",
    "    start = time.time()\n",
    "    print('epoch: 0', '/', EPOCHS, 'training loss: N/A')\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        losses, i = [], 0\n",
    "        for images, labels in get_batches_function(BATCH_SIZE):\n",
    "            i += 1\n",
    "            feed = {input_image: images,\n",
    "                   correct_label: labels,\n",
    "                   keep_prob: DROPOUT,\n",
    "                   learning_rate: LEARNING_RATE}\n",
    "\n",
    "            _, partial_loss = sess.run([train_optimizer, cross_entropy_loss], feed_dict = feed)\n",
    "            \n",
    "            print('-> iteration: ', i, '/', NUMBER_OF_IMAGES, 'partial loss: ', partial_loss)\n",
    "            losses.append(partial_loss)\n",
    "\n",
    "        training_loss = sum(losses) / len(losses)\n",
    "        \n",
    "        end = time.time()\n",
    "        training_time = end - start\n",
    "        \n",
    "        print('epoch: ', epoch + 1, '/', EPOCHS, 'training loss: ', training_loss)\n",
    "    print('training time: ', training_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_batches_function = generate_batch_function(TRAINING_DATA_DIRECTORY, IMAGE_SHAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from b'./data/vgg/variables/variables'\n",
      "epoch: 0 / 1 training loss: N/A\n",
      "---> iteration:  1 / 289 partial loss:  57.317\n",
      "---> iteration:  2 / 289 partial loss:  30.9959\n",
      "---> iteration:  3 / 289 partial loss:  46.8647\n",
      "---> iteration:  4 / 289 partial loss:  21.2122\n",
      "---> iteration:  5 / 289 partial loss:  21.4382\n",
      "---> iteration:  6 / 289 partial loss:  17.7256\n",
      "---> iteration:  7 / 289 partial loss:  14.3689\n",
      "---> iteration:  8 / 289 partial loss:  20.6739\n",
      "---> iteration:  9 / 289 partial loss:  14.4892\n",
      "---> iteration:  10 / 289 partial loss:  12.6492\n",
      "---> iteration:  11 / 289 partial loss:  12.45\n",
      "---> iteration:  12 / 289 partial loss:  11.0529\n",
      "---> iteration:  13 / 289 partial loss:  8.04862\n",
      "---> iteration:  14 / 289 partial loss:  7.4452\n",
      "---> iteration:  15 / 289 partial loss:  6.22143\n",
      "---> iteration:  16 / 289 partial loss:  6.0769\n",
      "---> iteration:  17 / 289 partial loss:  5.25337\n",
      "---> iteration:  18 / 289 partial loss:  5.76766\n",
      "---> iteration:  19 / 289 partial loss:  5.91477\n",
      "---> iteration:  20 / 289 partial loss:  5.7729\n",
      "---> iteration:  21 / 289 partial loss:  4.99731\n",
      "---> iteration:  22 / 289 partial loss:  5.159\n",
      "---> iteration:  23 / 289 partial loss:  3.60825\n",
      "---> iteration:  24 / 289 partial loss:  3.56498\n",
      "---> iteration:  25 / 289 partial loss:  3.49679\n",
      "---> iteration:  26 / 289 partial loss:  3.50722\n",
      "---> iteration:  27 / 289 partial loss:  3.76624\n",
      "---> iteration:  28 / 289 partial loss:  2.79808\n",
      "---> iteration:  29 / 289 partial loss:  2.91139\n",
      "---> iteration:  30 / 289 partial loss:  2.67964\n",
      "---> iteration:  31 / 289 partial loss:  3.32297\n",
      "---> iteration:  32 / 289 partial loss:  2.8098\n",
      "---> iteration:  33 / 289 partial loss:  3.14099\n",
      "---> iteration:  34 / 289 partial loss:  2.2615\n",
      "---> iteration:  35 / 289 partial loss:  2.31053\n",
      "---> iteration:  36 / 289 partial loss:  3.1044\n",
      "---> iteration:  37 / 289 partial loss:  2.34899\n",
      "---> iteration:  38 / 289 partial loss:  2.18367\n",
      "---> iteration:  39 / 289 partial loss:  2.23098\n",
      "---> iteration:  40 / 289 partial loss:  2.55469\n",
      "---> iteration:  41 / 289 partial loss:  1.97192\n",
      "---> iteration:  42 / 289 partial loss:  2.05759\n",
      "---> iteration:  43 / 289 partial loss:  2.26023\n",
      "---> iteration:  44 / 289 partial loss:  1.72863\n",
      "---> iteration:  45 / 289 partial loss:  1.67289\n",
      "---> iteration:  46 / 289 partial loss:  2.00901\n",
      "---> iteration:  47 / 289 partial loss:  1.97947\n",
      "---> iteration:  48 / 289 partial loss:  1.70741\n",
      "---> iteration:  49 / 289 partial loss:  1.59361\n",
      "---> iteration:  50 / 289 partial loss:  1.95143\n",
      "---> iteration:  51 / 289 partial loss:  1.65284\n",
      "---> iteration:  52 / 289 partial loss:  1.63053\n",
      "---> iteration:  53 / 289 partial loss:  1.63462\n",
      "---> iteration:  54 / 289 partial loss:  1.91466\n",
      "---> iteration:  55 / 289 partial loss:  1.60334\n",
      "---> iteration:  56 / 289 partial loss:  1.7984\n",
      "---> iteration:  57 / 289 partial loss:  1.3658\n",
      "---> iteration:  58 / 289 partial loss:  1.34848\n",
      "---> iteration:  59 / 289 partial loss:  1.41081\n",
      "---> iteration:  60 / 289 partial loss:  1.68008\n",
      "---> iteration:  61 / 289 partial loss:  1.83976\n",
      "---> iteration:  62 / 289 partial loss:  1.32012\n",
      "---> iteration:  63 / 289 partial loss:  1.70406\n",
      "---> iteration:  64 / 289 partial loss:  1.25774\n",
      "---> iteration:  65 / 289 partial loss:  1.51833\n",
      "---> iteration:  66 / 289 partial loss:  1.30343\n",
      "---> iteration:  67 / 289 partial loss:  1.3442\n",
      "---> iteration:  68 / 289 partial loss:  1.39699\n",
      "---> iteration:  69 / 289 partial loss:  1.76232\n",
      "---> iteration:  70 / 289 partial loss:  1.15854\n",
      "---> iteration:  71 / 289 partial loss:  1.08204\n",
      "---> iteration:  72 / 289 partial loss:  1.54899\n",
      "---> iteration:  73 / 289 partial loss:  1.18755\n",
      "---> iteration:  74 / 289 partial loss:  1.25477\n",
      "---> iteration:  75 / 289 partial loss:  1.53852\n",
      "---> iteration:  76 / 289 partial loss:  1.18874\n",
      "---> iteration:  77 / 289 partial loss:  1.29491\n",
      "---> iteration:  78 / 289 partial loss:  1.31052\n",
      "---> iteration:  79 / 289 partial loss:  1.36191\n",
      "---> iteration:  80 / 289 partial loss:  1.17585\n",
      "---> iteration:  81 / 289 partial loss:  1.23521\n",
      "---> iteration:  82 / 289 partial loss:  1.3541\n",
      "---> iteration:  83 / 289 partial loss:  1.08331\n",
      "---> iteration:  84 / 289 partial loss:  1.34243\n",
      "---> iteration:  85 / 289 partial loss:  1.31063\n",
      "---> iteration:  86 / 289 partial loss:  1.09889\n",
      "---> iteration:  87 / 289 partial loss:  1.29658\n",
      "---> iteration:  88 / 289 partial loss:  1.05368\n",
      "---> iteration:  89 / 289 partial loss:  1.31333\n",
      "---> iteration:  90 / 289 partial loss:  1.14886\n",
      "---> iteration:  91 / 289 partial loss:  1.26897\n",
      "---> iteration:  92 / 289 partial loss:  1.11522\n",
      "---> iteration:  93 / 289 partial loss:  1.17252\n",
      "---> iteration:  94 / 289 partial loss:  0.943311\n",
      "---> iteration:  95 / 289 partial loss:  0.991931\n",
      "---> iteration:  96 / 289 partial loss:  0.927176\n",
      "---> iteration:  97 / 289 partial loss:  1.19255\n",
      "---> iteration:  98 / 289 partial loss:  1.03329\n",
      "---> iteration:  99 / 289 partial loss:  1.05883\n",
      "---> iteration:  100 / 289 partial loss:  0.953398\n",
      "---> iteration:  101 / 289 partial loss:  0.948905\n",
      "---> iteration:  102 / 289 partial loss:  1.03716\n",
      "---> iteration:  103 / 289 partial loss:  1.03858\n",
      "---> iteration:  104 / 289 partial loss:  0.969154\n",
      "---> iteration:  105 / 289 partial loss:  1.10914\n",
      "---> iteration:  106 / 289 partial loss:  1.03333\n",
      "---> iteration:  107 / 289 partial loss:  0.9658\n",
      "---> iteration:  108 / 289 partial loss:  0.964498\n",
      "---> iteration:  109 / 289 partial loss:  1.07115\n",
      "---> iteration:  110 / 289 partial loss:  0.950515\n",
      "---> iteration:  111 / 289 partial loss:  0.940267\n",
      "---> iteration:  112 / 289 partial loss:  0.893761\n",
      "---> iteration:  113 / 289 partial loss:  0.976329\n",
      "---> iteration:  114 / 289 partial loss:  0.978176\n",
      "---> iteration:  115 / 289 partial loss:  0.966872\n",
      "---> iteration:  116 / 289 partial loss:  0.926623\n",
      "---> iteration:  117 / 289 partial loss:  1.0011\n",
      "---> iteration:  118 / 289 partial loss:  0.894366\n",
      "---> iteration:  119 / 289 partial loss:  0.838921\n",
      "---> iteration:  120 / 289 partial loss:  0.968357\n",
      "---> iteration:  121 / 289 partial loss:  0.846818\n",
      "---> iteration:  122 / 289 partial loss:  1.05114\n",
      "---> iteration:  123 / 289 partial loss:  0.902883\n",
      "---> iteration:  124 / 289 partial loss:  0.873666\n",
      "---> iteration:  125 / 289 partial loss:  0.876688\n",
      "---> iteration:  126 / 289 partial loss:  0.87954\n",
      "---> iteration:  127 / 289 partial loss:  0.95102\n",
      "---> iteration:  128 / 289 partial loss:  1.20149\n",
      "---> iteration:  129 / 289 partial loss:  0.904965\n",
      "---> iteration:  130 / 289 partial loss:  0.850331\n",
      "---> iteration:  131 / 289 partial loss:  0.842619\n",
      "---> iteration:  132 / 289 partial loss:  0.858489\n",
      "---> iteration:  133 / 289 partial loss:  0.860611\n",
      "---> iteration:  134 / 289 partial loss:  0.856967\n",
      "---> iteration:  135 / 289 partial loss:  0.815719\n",
      "---> iteration:  136 / 289 partial loss:  0.908516\n",
      "---> iteration:  137 / 289 partial loss:  0.865101\n",
      "---> iteration:  138 / 289 partial loss:  0.931522\n",
      "---> iteration:  139 / 289 partial loss:  0.967752\n",
      "---> iteration:  140 / 289 partial loss:  0.78326\n",
      "---> iteration:  141 / 289 partial loss:  0.863918\n",
      "---> iteration:  142 / 289 partial loss:  0.903941\n",
      "---> iteration:  143 / 289 partial loss:  0.959871\n",
      "---> iteration:  144 / 289 partial loss:  0.83004\n",
      "---> iteration:  145 / 289 partial loss:  0.808737\n",
      "---> iteration:  146 / 289 partial loss:  0.927911\n",
      "---> iteration:  147 / 289 partial loss:  0.864839\n",
      "---> iteration:  148 / 289 partial loss:  0.800267\n",
      "---> iteration:  149 / 289 partial loss:  1.00641\n",
      "---> iteration:  150 / 289 partial loss:  0.782462\n",
      "---> iteration:  151 / 289 partial loss:  0.832647\n",
      "---> iteration:  152 / 289 partial loss:  0.812087\n",
      "---> iteration:  153 / 289 partial loss:  0.932412\n",
      "---> iteration:  154 / 289 partial loss:  0.789022\n",
      "---> iteration:  155 / 289 partial loss:  0.806436\n",
      "---> iteration:  156 / 289 partial loss:  0.837031\n",
      "---> iteration:  157 / 289 partial loss:  0.85379\n",
      "---> iteration:  158 / 289 partial loss:  0.843096\n",
      "---> iteration:  159 / 289 partial loss:  0.830879\n",
      "---> iteration:  160 / 289 partial loss:  0.874855\n",
      "---> iteration:  161 / 289 partial loss:  0.827118\n",
      "---> iteration:  162 / 289 partial loss:  0.806816\n",
      "---> iteration:  163 / 289 partial loss:  0.870057\n",
      "---> iteration:  164 / 289 partial loss:  0.83108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> iteration:  165 / 289 partial loss:  0.858921\n",
      "---> iteration:  166 / 289 partial loss:  0.801933\n",
      "---> iteration:  167 / 289 partial loss:  0.787094\n",
      "---> iteration:  168 / 289 partial loss:  0.784124\n",
      "---> iteration:  169 / 289 partial loss:  0.771223\n",
      "---> iteration:  170 / 289 partial loss:  0.776431\n",
      "---> iteration:  171 / 289 partial loss:  0.878011\n",
      "---> iteration:  172 / 289 partial loss:  0.7807\n",
      "---> iteration:  173 / 289 partial loss:  0.86002\n",
      "---> iteration:  174 / 289 partial loss:  0.84983\n",
      "---> iteration:  175 / 289 partial loss:  0.841655\n",
      "---> iteration:  176 / 289 partial loss:  0.789239\n",
      "---> iteration:  177 / 289 partial loss:  0.786108\n",
      "---> iteration:  178 / 289 partial loss:  0.839303\n",
      "---> iteration:  179 / 289 partial loss:  0.766587\n",
      "---> iteration:  180 / 289 partial loss:  0.862978\n",
      "---> iteration:  181 / 289 partial loss:  0.734057\n",
      "---> iteration:  182 / 289 partial loss:  0.757243\n",
      "---> iteration:  183 / 289 partial loss:  0.822639\n",
      "---> iteration:  184 / 289 partial loss:  0.753469\n",
      "---> iteration:  185 / 289 partial loss:  0.801314\n",
      "---> iteration:  186 / 289 partial loss:  0.747581\n",
      "---> iteration:  187 / 289 partial loss:  0.76132\n",
      "---> iteration:  188 / 289 partial loss:  0.76881\n",
      "---> iteration:  189 / 289 partial loss:  0.794654\n",
      "---> iteration:  190 / 289 partial loss:  0.756619\n",
      "---> iteration:  191 / 289 partial loss:  0.82684\n",
      "---> iteration:  192 / 289 partial loss:  0.817448\n",
      "---> iteration:  193 / 289 partial loss:  0.750641\n",
      "---> iteration:  194 / 289 partial loss:  0.733453\n",
      "---> iteration:  195 / 289 partial loss:  0.79906\n",
      "---> iteration:  196 / 289 partial loss:  0.745141\n",
      "---> iteration:  197 / 289 partial loss:  0.747932\n",
      "---> iteration:  198 / 289 partial loss:  0.741953\n",
      "---> iteration:  199 / 289 partial loss:  0.748092\n",
      "---> iteration:  200 / 289 partial loss:  0.821721\n",
      "---> iteration:  201 / 289 partial loss:  0.723531\n",
      "---> iteration:  202 / 289 partial loss:  0.807688\n",
      "---> iteration:  203 / 289 partial loss:  0.807827\n",
      "---> iteration:  204 / 289 partial loss:  0.752553\n",
      "---> iteration:  205 / 289 partial loss:  0.803759\n",
      "---> iteration:  206 / 289 partial loss:  0.743195\n",
      "---> iteration:  207 / 289 partial loss:  0.77195\n",
      "---> iteration:  208 / 289 partial loss:  0.743692\n",
      "---> iteration:  209 / 289 partial loss:  0.713179\n",
      "---> iteration:  210 / 289 partial loss:  0.763634\n",
      "---> iteration:  211 / 289 partial loss:  0.756678\n",
      "---> iteration:  212 / 289 partial loss:  0.773006\n",
      "---> iteration:  213 / 289 partial loss:  0.725103\n",
      "---> iteration:  214 / 289 partial loss:  0.797041\n",
      "---> iteration:  215 / 289 partial loss:  0.827354\n",
      "---> iteration:  216 / 289 partial loss:  0.690903\n",
      "---> iteration:  217 / 289 partial loss:  0.734632\n",
      "---> iteration:  218 / 289 partial loss:  0.804797\n",
      "---> iteration:  219 / 289 partial loss:  0.711578\n",
      "---> iteration:  220 / 289 partial loss:  0.757285\n",
      "---> iteration:  221 / 289 partial loss:  0.725815\n",
      "---> iteration:  222 / 289 partial loss:  0.721887\n",
      "---> iteration:  223 / 289 partial loss:  0.716201\n",
      "---> iteration:  224 / 289 partial loss:  0.779714\n",
      "---> iteration:  225 / 289 partial loss:  0.763045\n",
      "---> iteration:  226 / 289 partial loss:  0.732483\n",
      "---> iteration:  227 / 289 partial loss:  0.766091\n",
      "---> iteration:  228 / 289 partial loss:  0.713249\n",
      "---> iteration:  229 / 289 partial loss:  0.772653\n",
      "---> iteration:  230 / 289 partial loss:  0.757187\n",
      "---> iteration:  231 / 289 partial loss:  0.754729\n",
      "---> iteration:  232 / 289 partial loss:  0.718888\n",
      "---> iteration:  233 / 289 partial loss:  0.721071\n",
      "---> iteration:  234 / 289 partial loss:  0.711642\n",
      "---> iteration:  235 / 289 partial loss:  0.71949\n",
      "---> iteration:  236 / 289 partial loss:  0.750875\n",
      "---> iteration:  237 / 289 partial loss:  0.718438\n",
      "---> iteration:  238 / 289 partial loss:  0.718652\n",
      "---> iteration:  239 / 289 partial loss:  0.738181\n",
      "---> iteration:  240 / 289 partial loss:  0.710007\n",
      "---> iteration:  241 / 289 partial loss:  0.790585\n",
      "---> iteration:  242 / 289 partial loss:  0.693339\n",
      "---> iteration:  243 / 289 partial loss:  0.715884\n",
      "---> iteration:  244 / 289 partial loss:  0.736292\n",
      "---> iteration:  245 / 289 partial loss:  0.71396\n",
      "---> iteration:  246 / 289 partial loss:  0.713729\n",
      "---> iteration:  247 / 289 partial loss:  0.733492\n",
      "---> iteration:  248 / 289 partial loss:  0.704794\n",
      "---> iteration:  249 / 289 partial loss:  0.714513\n",
      "---> iteration:  250 / 289 partial loss:  0.709216\n",
      "---> iteration:  251 / 289 partial loss:  0.707096\n",
      "---> iteration:  252 / 289 partial loss:  0.724914\n",
      "---> iteration:  253 / 289 partial loss:  0.722759\n",
      "---> iteration:  254 / 289 partial loss:  0.697485\n",
      "---> iteration:  255 / 289 partial loss:  0.706299\n",
      "---> iteration:  256 / 289 partial loss:  0.691551\n",
      "---> iteration:  257 / 289 partial loss:  0.753584\n",
      "---> iteration:  258 / 289 partial loss:  0.700487\n",
      "---> iteration:  259 / 289 partial loss:  0.721691\n",
      "---> iteration:  260 / 289 partial loss:  0.725937\n",
      "---> iteration:  261 / 289 partial loss:  0.718655\n",
      "---> iteration:  262 / 289 partial loss:  0.693238\n",
      "---> iteration:  263 / 289 partial loss:  0.70705\n",
      "---> iteration:  264 / 289 partial loss:  0.728457\n",
      "---> iteration:  265 / 289 partial loss:  0.727233\n",
      "---> iteration:  266 / 289 partial loss:  0.684091\n",
      "---> iteration:  267 / 289 partial loss:  0.73653\n",
      "---> iteration:  268 / 289 partial loss:  0.708911\n",
      "---> iteration:  269 / 289 partial loss:  0.698498\n",
      "---> iteration:  270 / 289 partial loss:  0.697157\n",
      "---> iteration:  271 / 289 partial loss:  0.698574\n",
      "---> iteration:  272 / 289 partial loss:  0.688833\n",
      "---> iteration:  273 / 289 partial loss:  0.694511\n",
      "---> iteration:  274 / 289 partial loss:  0.732319\n",
      "---> iteration:  275 / 289 partial loss:  0.686007\n",
      "---> iteration:  276 / 289 partial loss:  0.693447\n",
      "---> iteration:  277 / 289 partial loss:  0.73252\n",
      "---> iteration:  278 / 289 partial loss:  0.695917\n",
      "---> iteration:  279 / 289 partial loss:  0.686136\n",
      "---> iteration:  280 / 289 partial loss:  0.682557\n",
      "---> iteration:  281 / 289 partial loss:  0.672274\n",
      "---> iteration:  282 / 289 partial loss:  0.699563\n",
      "---> iteration:  283 / 289 partial loss:  0.694304\n",
      "---> iteration:  284 / 289 partial loss:  0.710108\n",
      "---> iteration:  285 / 289 partial loss:  0.684748\n",
      "---> iteration:  286 / 289 partial loss:  0.674755\n",
      "---> iteration:  287 / 289 partial loss:  0.732662\n",
      "---> iteration:  288 / 289 partial loss:  0.699653\n",
      "---> iteration:  289 / 289 partial loss:  0.688676\n",
      "epoch:  1 / 1 training loss:  2.18345296362\n",
      "training time:  844.5610649585724\n",
      "Training Finished. Saving test images to: ./runs/epochs 1\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:    \n",
    "    image_input, keep_prob, layer3, layer4, layer7 = load_vgg(session, VGG_PATH)\n",
    "    model_output = layers(layer3, layer4, layer7, NUMBER_OF_CLASSES)\n",
    "    \n",
    "    logits, train_optimizer, cross_entropy_loss = optimize(model_output, correct_label, learning_rate, NUMBER_OF_CLASSES)\n",
    "    \n",
    "    session.run([tf.global_variables_initializer(), tf.local_variables_initializer()])\n",
    "    train_nn(session, EPOCHS, BATCH_SIZE, get_batches_function, train_optimizer, cross_entropy_loss, image_input, correct_label, keep_prob, learning_rate)\n",
    "    \n",
    "    save_inference_samples(RUNS_DIRECTORY, DATA_DIRECTORY, session, IMAGE_SHAPE, logits, keep_prob, image_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
